import asyncio
import numpy as np
import pickle
import pprint
import argparse
from collections import deque

from event_model import RunRouter
from event_model import DocumentRouter
from event_model import unpack_event_page
from bluesky.run_engine import Dispatcher, DocumentNames
from nmf import decomposition, example_plot
import matplotlib.pyplot as plt
import json
from bluesky.utils import install_qt_kicker

import databroker


class RemoteDispatcher(Dispatcher):
    """
    Dispatch documents received over the network from a 0MQ proxy.
    Parameters
    ----------
    address : tuple
        Address of a running 0MQ proxy, given either as a string like
        ``'127.0.0.1:5567'`` or as a tuple like ``('127.0.0.1', 5567)``
    prefix : bytes, optional
        User-defined bytestring used to distinguish between multiple
        Publishers. If set, messages without this prefix will be ignored.
        If unset, no mesages will be ignored.
    loop : zmq.asyncio.ZMQEventLoop, optional
    zmq : object, optional
        By default, the 'zmq' module is imported and used. Anything else
        mocking its interface is accepted.
    zmq_asyncio : object, optional
        By default, the 'zmq.asyncio' module is imported and used. Anything
        else mocking its interface is accepted.
    deserializer: function, optional
        optional function to deserialize data. Default is pickle.loads
    Examples
    --------
    Print all documents generated by remote RunEngines.
    >>> d = RemoteDispatcher(('localhost', 5568))
    >>> d.subscribe(print)
    >>> d.start()  # runs until interrupted
    """

    def __init__(
        self,
        address,
        *,
        prefix=b"",
        loop=None,
        zmq=None,
        zmq_asyncio=None,
        deserializer=pickle.loads,
    ):
        if isinstance(prefix, str):
            raise ValueError("prefix must be bytes, not string")
        if b" " in prefix:
            raise ValueError("prefix {!r} may not contain b' '".format(prefix))
        self._prefix = prefix
        if zmq is None:
            import zmq
        if zmq_asyncio is None:
            import zmq.asyncio as zmq_asyncio
        if isinstance(address, str):
            address = address.split(":", maxsplit=1)
        self._deserializer = deserializer
        self.address = (address[0], int(address[1]))

        if loop is None:
            loop = zmq_asyncio.ZMQEventLoop()
        self.loop = loop
        asyncio.set_event_loop(self.loop)
        self._context = zmq_asyncio.Context()
        self._socket = self._context.socket(zmq.SUB)
        url = "tcp://%s:%d" % self.address
        self._socket.connect(url)
        self._socket.setsockopt_string(zmq.SUBSCRIBE, "")
        self._task = None
        self.closed = False

        super().__init__()

    async def _poll(self):
        our_prefix = self._prefix  # local var to save an attribute lookup
        while True:
            message = await self._socket.recv()
            prefix, name, doc = message.split(b" ", 2)
            name = name.decode()
            if (not our_prefix) or prefix == our_prefix:
                try:
                    doc = self._deserializer(doc)
                    self.loop.call_soon(self.process, DocumentNames[name], doc)
                except Exception as e:
                    print(f"something bad happened with a {name} document")
                    print(e)

    def start(self):
        if self.closed:
            raise RuntimeError(
                "This RemoteDispatcher has already been "
                "started and interrupted. Create a fresh "
                "instance with {}".format(repr(self))
            )
        try:
            self._task = self.loop.create_task(self._poll())
            self.loop.run_forever()
        except BaseException:
            self.stop()
            raise

    def stop(self):
        if self._task is not None:
            self._task.cancel()
            self.loop.stop()
        self._task = None
        self.closed = True


class Accumulator(DocumentRouter):
    def __init__(self, max_N=1_000, event_filter=None):
        self._event_cache = deque(maxlen=max_N)
        # This can not be subscribed to the RE (due to Qt + thread issues)
        self.fig = plt.figure()
        self.fig.canvas.manager.show()
        self.update_plot = True
        if event_filter is None:
            event_filter = lambda doc: True
        self.event_filter = event_filter

    def event_page(self, doc):
        data = doc["data"]
        if "q" not in data or "mean" not in data:
            return

        # this will mix across runs blindly!
        for doc in unpack_event_page(doc):
            self._event_cache.append(doc)

        if self.update_plot:
            self.redraw_plot()

        print(f"Currently has {len(self._event_cache)} datasets")

    def redraw_plot(self):

        if len(self._event_cache) == 0:
            return

        Is = np.array(
            [d["data"]["mean"] for d in self._event_cache if self.event_filter(d)]
        )
        Qs = np.array(
            [d["data"]["q"] for d in self._event_cache if self.event_filter(d)]
        )
        sub_Q, sub_I, alphas = decomposition(
            Qs,
            Is,
            q_range=(2.0, 4.0),
            n_components=3,
            bkg_removal=8,
            normalize=True,
        )

        # nuke everything
        self.fig.clf()
        # make new axes
        axes = self.fig.subplots(1, 4)
        example_plot(sub_Q, sub_I, alphas, axes=axes[:3], sax=axes[3], summary_fig=True)
        self.fig.canvas.draw_idle()
        self.fig.canvas.flush_events()


arg_parser = argparse.ArgumentParser()

# publish 0MQ messages at XPD from xf28id2-ca1:5577
# subscribe to 0MQ messages at XPD from xf28id2-ca1:5578
arg_parser.add_argument("--zmq-host", type=str, default="xf28id2-ca1")
arg_parser.add_argument("--zmq-subscribe-port", type=int, default=5578)
arg_parser.add_argument("--zmq-subscribe-prefix", type=str, default="an")

# info about how to back-fill
arg_parser.add_argument("--catalog-name", type=str, default="xpd")
arg_parser.add_argument("--catalog-query", type=str, default="{}")

args = arg_parser.parse_args()

pprint.pprint(vars(args))

# this process listens for 0MQ messages with prefix "an" (from xpdan)
d = RemoteDispatcher(
    f"{args.zmq_host}:{args.zmq_subscribe_port}",
    prefix=args.zmq_subscribe_prefix.encode(),
)

query = json.loads(args.catalog_query)

accumulator = Accumulator()

if query:
    cat = databroker.catalog[args.catalog_name]
    search_results = cat.search(query)
    accumulator.update_plot = False
    for uid in search_results:
        h = cat[uid]
        for name, doc in h.canonical(fill="no"):
            accumulator(name, doc)
    accumulator.update_plot = True
    accumulator.redraw_plot()


def integration_accumulator(name, start_doc):
    print(f"analysis stage: {start_doc.get('analysis_stage')}")
    if start_doc.get("analysis_stage", "") == "integration":
        print("got integration start document")
        return [accumulator], []
    return [], []


rr = RunRouter([integration_accumulator])
d.subscribe(rr)
# force qt import
import matplotlib.backends.backend_qt5

install_qt_kicker()
print(f"NMF CONSUMER IS LISTENING ON {args.zmq_subscribe_prefix.encode()}")
d.start()
